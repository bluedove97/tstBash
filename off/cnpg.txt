https://velog.io/@kubernetes/Cloud-Native-PostgreSQL-1
https://velog.io/@xgro/doik2-03
https://sharing-for-us.tistory.com/15

# 목표
$ helm repo add cnpg-kbr https://cr.smap-beta.samsungds.net/chartrepo/cnpg-kbr
$ helm repo list
$ helm search repo cnpg-kbr
cnpg-kbr/cloudnative-pg 0.21.4          1.23.1          CloudNativePG Operator Helm Chart
cnpg-kbr/cluster        0.0.9                           Deploys and manages a CloudNativePG cluster and...
# cnpg-kbr-values.yaml 파일 작성
cat > cnpg-kbr-values.yaml
nodeSelector: {node-role.kubernetes.io/control-plane=}
tolerations: [{key: node-role.kubernetes.io/control-plane, operator: Exists, effect: NoSchedule}]
# CloudNative-PG Operator 설치
    helm -n cnpg-system install cnpg-kbr cnpg-kbr/cloudnative-pg -f ./cnpg-kbr-values.yaml
    or
    helm upgrade --install cnpg-kbr -n cnpg-system --create-namespace cnpg/cloudnative-pg -f cnpg-kbr-values.haml
	


cat > mycluster1.yaml
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: cluster-example
spec:
  instances: 3
  storage:
    size: 1Gi
	
	
	
kubectl apply -f ./mycluster1.yaml
kubectl get cluster
kubectl get deploy,po -n cnpg-system
kubectl get crd | grep cnpg



# PostgreSQL Cluster 구성
cat > cnpg-app-cluster.yaml
~~
imageName: ghcr.io/cloudnative-pg/postgresql:15.3
monitoring:
  enablePodMonitor: true
~~
$ kubectl create ns cnpg-app
$ kubectl apply -n cnpg-app -f cnpg-app-cluster.yaml
$ kubectl get pdb,po,svc,pvc -n cnpg-app -owide



----------------------------------------- HOME -----------------------------------
# REPO 추가
$ helm repo add cnpg https://cloudnative-pg.github.io/charts


$ helm search repo cnpg
NAME               	CHART VERSION	APP VERSION	DESCRIPTION                                       
cnpg/cnpg-sandbox  	0.6.1        	1.17.1     	A sandbox for CloudNativePG                       
cnpg/cloudnative-pg	0.21.4       	1.23.1     	CloudNativePG Operator Helm Chart                 
cnpg/cluster       	0.0.9        	           	Deploys and manages a CloudNativePG cluster and...
cnpg/pgbench       	0.1.0        	           	A Helm chart that starts a CNPG Cluster and exe...




cat > cnpg-values.yaml
nodeSelector: {node-role.kubernetes.io/control-plane=''}
tolerations: [{key: node-role.kubernetes.io/control-plane, operator: Exists, effect: NoSchedule}]

# CloudNative-PG Operator 설치
helm upgrade --install cnpg -n cnpg-system --create-namespace cnpg/cloudnative-pg -f cnpg-values.haml
helm upgrade --install cnpg -n cnpg-system --create-namespace cnpg/cloudnative-pg -f ./cnpg-values.yaml
Error: UPGRADE FAILED: failed to create resource: Deployment.apps "cnpg-cloudnative-pg" is invalid: spec.template.spec.nodeSelector: Invalid value: "node-role.kubernetes.io/control-plane=''": name part must consist of alphanumeric characters, '-', '_' or '.', and must start and end with an alphanumeric character (e.g. 'MyName',  or 'my.name',  or '123-abc', regex used for validation is '([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9]')


cat > cnpg-values.yaml
nodeSelector: {node-role.kubernetes.io/control-plane: }
tolerations: [{key: node-role.kubernetes.io/control-plane, operator: Exists, effect: NoSchedule}]

#다시
helm upgrade --install cnpg -n cnpg-system --create-namespace cnpg/cloudnative-pg -f ./cnpg-values.yaml
```
  Release "cnpg" has been upgraded. Happy Helming!
  NAME: cnpg
  LAST DEPLOYED: Thu Jun 13 23:02:43 2024
  NAMESPACE: cnpg-system
  STATUS: deployed
  REVISION: 6
  TEST SUITE: None
  NOTES:
  CloudNativePG operator should be installed in namespace "cnpg-system".
  You can now create a PostgreSQL cluster with 3 nodes in the current namespace as follows:
  
  cat <<EOF | kubectl apply -f -
  # Example of PostgreSQL cluster
  apiVersion: postgresql.cnpg.io/v1
  kind: Cluster
  metadata:
    name: cluster-example
  spec:
    instances: 3
    storage:
      size: 1Gi
  EOF

  kubectl get cluster
```

$ kubectl get crd | grep cnpg
```
backups.postgresql.cnpg.io                2024-06-13T13:38:24Z
clusterimagecatalogs.postgresql.cnpg.io   2024-06-13T13:38:24Z
clusters.postgresql.cnpg.io               2024-06-13T13:38:24Z
imagecatalogs.postgresql.cnpg.io          2024-06-13T13:38:24Z
poolers.postgresql.cnpg.io                2024-06-13T13:38:24Z
scheduledbackups.postgresql.cnpg.io       2024-06-13T13:38:24Z
```


$ kubectl get all -n cnpg-system 
```
NAME                                       READY   STATUS    RESTARTS       AGE
pod/cnpg-cloudnative-pg-55b577df96-v5hhk   1/1     Running   1 (144m ago)   2d21h

NAME                           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
service/cnpg-webhook-service   ClusterIP   10.96.165.126   <none>        443/TCP   2d21h

NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/cnpg-cloudnative-pg   1/1     1            1           2d21h

NAME                                             DESIRED   CURRENT   READY   AGE
replicaset.apps/cnpg-cloudnative-pg-55b577df96   1         1         1       2d21h
```


# 로컬시스템엔 storageclass 세팅이 안되어있으므로, 사용할 pv를 3개 미리 만들어둔다
cat > pv1.yaml 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv1
spec:
  capacity:
    storage: 1Gi
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce
  #storageClassName: manual
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: /mnt/pv1


cat > pv2.yaml 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv2
spec:
  capacity:
    storage: 1Gi
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce
  #storageClassName: manual
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: /mnt/pv2

cat > pv3.yaml 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv1
spec:
  capacity:
    storage: 1Gi
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce
  #storageClassName: manual
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: /mnt/pv3



$ kubectl create ns cnpg-app
cat > cnpg-app-cluster.yaml
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: cnpg-app-cluster
  namespace: cnpg-app
spec:
  imageName: ghcr.io/cloudnative-pg/postgresql:15.3
  instances: 3
  storage:
    size: 1Gi
  postgresql:
    parameters:
      max_worker_processes: "40"
      timezone: "Asia/Seoul"
    pg_hba:
      - host all postgres all trust
  primaryUpdateStrategy: unsupervised
  enableSuperuserAccess: true
  bootstrap:
    initdb:
      database: app
      encoding: UTF8
      localeCType: C
      localeCollate: C
      owner: app
  monitoring:
    enablePodMonitor: true

	
$ kubectl get cluster -n cnpg-app 
NAME               AGE   INSTANCES   READY   STATUS                     PRIMARY
cnpg-app-cluster   14m   3           3       Cluster in healthy state   cnpg-app-cluster-1

$ kubectl get pdb,po,svc,pvc -n cnpg-app -owide
```
NAME                                                  MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
poddisruptionbudget.policy/cnpg-app-cluster           1               N/A               1                     6m15s
poddisruptionbudget.policy/cnpg-app-cluster-primary   1               N/A               0                     6m15s

NAME                     READY   STATUS    RESTARTS   AGE     IP          NODE                NOMINATED NODE   READINESS GATES
pod/cnpg-app-cluster-1   1/1     Running   0          3m5s    10.32.0.2   node1.example.com   <none>           <none>
pod/cnpg-app-cluster-2   1/1     Running   0          2m39s   10.40.0.1   node2.example.com   <none>           <none>
pod/cnpg-app-cluster-3   1/1     Running   0          2m12s   10.32.0.3   node1.example.com   <none>           <none>

NAME                          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE     SELECTOR
service/cnpg-app-cluster-r    ClusterIP   10.107.129.14    <none>        5432/TCP   6m15s   cnpg.io/cluster=cnpg-app-cluster,cnpg.io/podRole=instance
service/cnpg-app-cluster-ro   ClusterIP   10.104.136.103   <none>        5432/TCP   6m15s   cnpg.io/cluster=cnpg-app-cluster,role=replica
service/cnpg-app-cluster-rw   ClusterIP   10.100.108.109   <none>        5432/TCP   6m15s   cnpg.io/cluster=cnpg-app-cluster,role=primary

NAME                                       STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE     VOLUMEMODE
persistentvolumeclaim/cnpg-app-cluster-1   Bound    pv1      1Gi        RWO                           6m15s   Filesystem
persistentvolumeclaim/cnpg-app-cluster-2   Bound    pv2      1Gi        RWO                           2m54s   Filesystem
persistentvolumeclaim/cnpg-app-cluster-3   Bound    pv3      1Gi        RWO                           2m27s   Filesystem
```




# nodeport 구성
cat > cnpg-app-np-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: cnpg-app-np-svc
  namespace: cnpg-app
spec:
  type: NodePort
  selector:
    cnpg.io/cluster: cnpg-app-cluster
    role: primary
  ports:
  - name: postgres
    port: 5432
    protocol: TCP
    targetPort: 5432
    nodePort: 30101



# primary pod 접속

kubectl get -n cnpg-app secrets cnpg-app-cluster-superuser -o jsonpath={.data.username} | base64 -d
postgres

kubectl get -n cnpg-app secrets cnpg-app-cluster-superuser -o jsonpath={.data.password} | base64 -d 
uhza6mq5yQkbPF2wXkYFfpFLaZRFM0zxNzMuCMDp42weHG44lwBwwQnUV9wR5vmf

kubectl exec -it -n cnpg-app pod/cnpg-app-cluster-1 -- bash




# client test용 pod 생성
cat > cnpg-app-client.yaml
apiVersion: v1
kind: Pod
metadata:
  name: cnpg-app-client
  namespace: cnpg-app
  labels:
    app: cnpg-app-client
spec:
  containers:
  - name: cnpg-app-client-cntr
    image: 
    command: ["tail"]
    args: ["-f", "/dev/null"]


kubectl exec -it -n cnpg-app pod/cnpg-app-client -- bash



kubectl exec -it pod/cnpg-app-cluster-1 -- psql -U postgres -h cnpg-app-cluster-rw -p 5432

psql -U postgres -h cnpg-app-cluster-rw -p 5432



CREATE USER cnpg_user PASSWORD 'cnpg1234' SUPERUSER;

ALTER USER cnpg_user CREATEDB;


CREATE DATABASE cnpg WITH OWNER cnpg_user ENCODING 'UTF8';

GRANT ALL PRIVILEGES ON DATABASE cnpg TO cnpg_user;



psql -d cnpg -u cnpg_user -h cnpg-app-cluster-rw -p 5432

CREATE TABLE public.test1 (
	id bigserial NOT NULL,
	"date" timestamp DEFAULT CURRENT_TIMESTAMP NULL,
	cont varchar(100) NULL,
	CONSTRAINT test1_pk PRIMARY KEY (id)
);
INSERT INTO test1 (cont) VALUES('from pod')

psql -d cnpg -U cnpg_user -h cnpg-app-cluster-rw -p 5432 -c "select * from test1"
psql -d cnpg -U cnpg_user -h cnpg-app-cluster-rw -p 5432 -c "INSERT INTO test1 (cont) VALUES('from pod')"
psql -d cnpg -U postgres -h cnpg-app-cluster-rw -p 5432 -c "INSERT INTO test1 (cont) VALUES('from pod')"

kubectl exec -n cnpg-app -it pod/cnpg-app-cluster-1 -- psql -d cnpg -U postgres -h cnpg-app-cluster-rw -p 5432 -c "INSERT INTO test1 (cont) VALUES('from pod')"


입력
cat > test-run.sh
#!/bin/bash
  
for var in {1..100}
do
  echo $var
  kubectl exec -n cnpg-app -it pod/cnpg-app-cluster-1 -- psql -d cnpg -U postgres -h cnpg-app-cluster-rw -p 5432 -c "INSERT INTO test1 (cont) VALUES('from step $var')"
  sleep 0.1
done

모니터링 1
watch kubectl get pdb,po,svc,pvc -n cnpg-app -owide

모니터링 2
while true; do kubectl exec -it -n cnpg-app pod/cnpg-app-client -- psql -U postgres -h cnpg-app-cluster-ro -p 5432 -d cnpg -c "select count(*) from test1"; sleep 1; done;


테스트 파드 1 삭제



docker pull ghcr.io/cloudnative-pg/postgresql:15.3
docker tag ghcr.io/cloudnative-pg/postgresql:15.3 bluedove97/cloudnative-pg-postgresql:15.3
docker push bluedove97/cloudnative-pg-postgresql:15.3


docker pull bitnami/postgresql:15
docker tag bitnami/postgresql:15 bluedove97/postgresql:15
docker push bluedove97/postgresql:15
